[aot_autograd.py:2815 INFO] TRACED GRAPH
 ===== Joint graph 0 =====
 <eval_with_key>.7 from /scratch/network/OPEN-CATALYST/python/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:477 in wrapped class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: bf16[32768, 128256], primals_2: i64[32768], tangents_1: bf16[32768], = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
        # File: /scratch/network/OPEN-CATALYST/bench_v0.py:28, code: return loss_module(logits, targets)
        convert_element_type: f32[32768, 128256] = torch.ops.prims.convert_element_type.default(primals_1, torch.float32);  primals_1 = None
        amax: f32[32768, 1] = torch.ops.aten.amax.default(convert_element_type, [1], True)
        sub: f32[32768, 128256] = torch.ops.aten.sub.Tensor(convert_element_type, amax);  convert_element_type = amax = None
        exp: f32[32768, 128256] = torch.ops.aten.exp.default(sub)
        sum_1: f32[32768, 1] = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
        log: f32[32768, 1] = torch.ops.aten.log.default(sum_1);  sum_1 = None
        sub_1: f32[32768, 128256] = torch.ops.aten.sub.Tensor(sub, log);  sub = log = None
        convert_element_type_1: bf16[32768, 128256] = torch.ops.prims.convert_element_type.default(sub_1, torch.bfloat16);  sub_1 = None
        ne: b8[32768] = torch.ops.aten.ne.Scalar(primals_2, -100)
        scalar_tensor: i64[] = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
        where: i64[32768] = torch.ops.aten.where.self(ne, primals_2, scalar_tensor);  ne = scalar_tensor = None
        unsqueeze: i64[32768, 1] = torch.ops.aten.unsqueeze.default(where, 1);  where = None
        gather: bf16[32768, 1] = torch.ops.aten.gather.default(convert_element_type_1, 1, unsqueeze);  unsqueeze = None
        squeeze: bf16[32768] = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
        neg: bf16[32768] = torch.ops.aten.neg.default(squeeze);  squeeze = None
        ne_1: b8[32768] = torch.ops.aten.ne.Scalar(primals_2, -100)
        scalar_tensor_1: bf16[] = torch.ops.aten.scalar_tensor.default(0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0))
        where_1: bf16[32768] = torch.ops.aten.where.self(ne_1, neg, scalar_tensor_1);  ne_1 = neg = scalar_tensor_1 = None
        unsqueeze_1: i64[32768, 1] = torch.ops.aten.unsqueeze.default(primals_2, 1);  primals_2 = None
        ne_2: b8[32768, 1] = torch.ops.aten.ne.Scalar(unsqueeze_1, -100)
        scalar_tensor_2: i64[] = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
        where_2: i64[32768, 1] = torch.ops.aten.where.self(ne_2, unsqueeze_1, scalar_tensor_2);  ne_2 = scalar_tensor_2 = None
        full_1: bf16[32768, 128256] = torch.ops.aten.full.default([32768, 128256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        scatter: bf16[32768, 128256] = torch.ops.aten.scatter.value(full_1, 1, where_2, -1.0);  full_1 = where_2 = None
        unsqueeze_2: bf16[32768, 1] = torch.ops.aten.unsqueeze.default(tangents_1, 1);  tangents_1 = None
        ne_3: b8[32768, 1] = torch.ops.aten.ne.Scalar(unsqueeze_1, -100);  unsqueeze_1 = None
        scalar_tensor_3: bf16[] = torch.ops.aten.scalar_tensor.default(0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0))
        where_3: bf16[32768, 1] = torch.ops.aten.where.self(ne_3, unsqueeze_2, scalar_tensor_3);  ne_3 = unsqueeze_2 = scalar_tensor_3 = None
        mul: bf16[32768, 128256] = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
        convert_element_type_2: f32[32768, 128256] = torch.ops.prims.convert_element_type.default(mul, torch.float32);  mul = None
        convert_element_type_3: f32[32768, 128256] = torch.ops.prims.convert_element_type.default(convert_element_type_1, torch.float32);  convert_element_type_1 = None
        exp_1: f32[32768, 128256] = torch.ops.aten.exp.default(convert_element_type_3);  convert_element_type_3 = None
        sum_2: f32[32768, 1] = torch.ops.aten.sum.dim_IntList(convert_element_type_2, [1], True)
        mul_1: f32[32768, 128256] = torch.ops.aten.mul.Tensor(exp_1, sum_2);  exp_1 = sum_2 = None
        sub_2: f32[32768, 128256] = torch.ops.aten.sub.Tensor(convert_element_type_2, mul_1);  convert_element_type_2 = mul_1 = None
        convert_element_type_4: bf16[32768, 128256] = torch.ops.prims.convert_element_type.default(sub_2, torch.bfloat16);  sub_2 = None
        return pytree.tree_unflatten([where_1, convert_element_type_4, None], self._out_spec)
        

[aot_autograd.py:2899 INFO] TRACED GRAPH
 ===== Forward graph 0 =====
 <eval_with_key>.154 class GraphModule(torch.nn.Module):
    def forward(self, primals_1: bf16[32768, 128256], primals_2: i64[32768]):
        # File: /scratch/network/OPEN-CATALYST/bench_v0.py:28, code: return loss_module(logits, targets)
        convert_element_type: f32[32768, 128256] = torch.ops.prims.convert_element_type.default(primals_1, torch.float32);  primals_1 = None
        amax: f32[32768, 1] = torch.ops.aten.amax.default(convert_element_type, [1], True)
        sub: f32[32768, 128256] = torch.ops.aten.sub.Tensor(convert_element_type, amax);  convert_element_type = amax = None
        exp: f32[32768, 128256] = torch.ops.aten.exp.default(sub)
        sum_1: f32[32768, 1] = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
        log: f32[32768, 1] = torch.ops.aten.log.default(sum_1);  sum_1 = None
        sub_1: f32[32768, 128256] = torch.ops.aten.sub.Tensor(sub, log);  sub = log = None
        convert_element_type_1: bf16[32768, 128256] = torch.ops.prims.convert_element_type.default(sub_1, torch.bfloat16);  sub_1 = None
        ne: b8[32768] = torch.ops.aten.ne.Scalar(primals_2, -100)
        full_default: i64[] = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where: i64[32768] = torch.ops.aten.where.self(ne, primals_2, full_default);  full_default = None
        unsqueeze: i64[32768, 1] = torch.ops.aten.unsqueeze.default(where, 1);  where = None
        gather: bf16[32768, 1] = torch.ops.aten.gather.default(convert_element_type_1, 1, unsqueeze);  unsqueeze = None
        squeeze: bf16[32768] = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
        neg: bf16[32768] = torch.ops.aten.neg.default(squeeze);  squeeze = None
        full_default_1: bf16[] = torch.ops.aten.full.default([], 0.0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        where_1: bf16[32768] = torch.ops.aten.where.self(ne, neg, full_default_1);  ne = neg = full_default_1 = None
        return [where_1, primals_2, convert_element_type_1]
        

[aot_autograd.py:2900 INFO] TRACED GRAPH
 ===== Backward graph 0 =====
 <eval_with_key>.155 class GraphModule(torch.nn.Module):
    def forward(self, primals_2: i64[32768], convert_element_type_1: bf16[32768, 128256], tangents_1: bf16[32768]):
        # File: /scratch/network/OPEN-CATALYST/bench_v0.py:28, code: return loss_module(logits, targets)
        full_default: i64[] = torch.ops.aten.full.default([], 0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        full_default_1: bf16[] = torch.ops.aten.full.default([], 0.0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        unsqueeze_1: i64[32768, 1] = torch.ops.aten.unsqueeze.default(primals_2, 1);  primals_2 = None
        ne_2: b8[32768, 1] = torch.ops.aten.ne.Scalar(unsqueeze_1, -100)
        where_2: i64[32768, 1] = torch.ops.aten.where.self(ne_2, unsqueeze_1, full_default);  unsqueeze_1 = full_default = None
        full_default_3: bf16[32768, 128256] = torch.ops.aten.full.default([32768, 128256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        scatter: bf16[32768, 128256] = torch.ops.aten.scatter.value(full_default_3, 1, where_2, -1.0);  full_default_3 = where_2 = None
        unsqueeze_2: bf16[32768, 1] = torch.ops.aten.unsqueeze.default(tangents_1, 1);  tangents_1 = None
        where_3: bf16[32768, 1] = torch.ops.aten.where.self(ne_2, unsqueeze_2, full_default_1);  ne_2 = unsqueeze_2 = full_default_1 = None
        mul: bf16[32768, 128256] = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
        convert_element_type_2: f32[32768, 128256] = torch.ops.prims.convert_element_type.default(mul, torch.float32);  mul = None
        convert_element_type_3: f32[32768, 128256] = torch.ops.prims.convert_element_type.default(convert_element_type_1, torch.float32);  convert_element_type_1 = None
        exp_1: f32[32768, 128256] = torch.ops.aten.exp.default(convert_element_type_3);  convert_element_type_3 = None
        sum_2: f32[32768, 1] = torch.ops.aten.sum.dim_IntList(convert_element_type_2, [1], True)
        mul_1: f32[32768, 128256] = torch.ops.aten.mul.Tensor(exp_1, sum_2);  exp_1 = sum_2 = None
        sub_2: f32[32768, 128256] = torch.ops.aten.sub.Tensor(convert_element_type_2, mul_1);  convert_element_type_2 = mul_1 = None
        convert_element_type_4: bf16[32768, 128256] = torch.ops.prims.convert_element_type.default(sub_2, torch.bfloat16);  sub_2 = None
        return [convert_element_type_4, None]
        

